"""
[ğŸ“„ uploader_service.py - File ì„œë¹„ìŠ¤ êµ¬í˜„ì²´]

ì„¤ëª…:
- íŒŒì¼ ì—…ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
- ì‚¬ìš©ì ID, íŒŒì¼, í† í”½ëª…ì„ ë°›ì•„ ì—…ë¡œë“œ ì²˜ë¦¬

ì£¼ìš” ì—°ë™:
- UploaderInterface (ì¸í„°í˜ì´ìŠ¤)
"""

from fastapi import UploadFile
from app.domains.file.services.interfaces.uploader_interface import UploaderInterface
from app.domains.file.schemas.upload import UploadResponse
import aiofiles
import uuid
import os
from app.common.config import settings
from app.domains.file.services.impl.minio_memory_client import MinioMemoryClient
from app.domains.file.services.impl.minio_prod_client import MinioProdClient

from app.common.logging import logger
from app.common.kafka.producer import KafkaMessageProducer

class UploaderService(UploaderInterface):
    """
    WHAT: íŒŒì¼ ì—…ë¡œë“œ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
    WHY: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ì„ S3(Minio) ë˜ëŠ” ë©”ëª¨ë¦¬ë¡œ ì €ì¥í•˜ê³ , ë©”íƒ€ë°ì´í„°ë¥¼ Kafkaë¡œ ë°œí–‰
    """

    async def upload_file(self, file: UploadFile):
        """
        WHAT: íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ë©”ì„œë“œ
        WHY: ì—…ë¡œë“œ íŒŒì¼ì„ í™˜ê²½ì— ë”°ë¼ S3 ë˜ëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥, ë©”íƒ€ë°ì´í„°ëŠ” Kafkaë¡œ ë°œí–‰
        Args:
            file (UploadFile): ì—…ë¡œë“œí•  íŒŒì¼ ê°ì²´
        Returns:
            UploadResponse: ì—…ë¡œë“œ ê²°ê³¼ ì •ë³´
        Raises:
            Exception: ì—…ë¡œë“œ/ë©”íƒ€ë°ì´í„° ë°œí–‰ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        """
        # WHAT: ì—…ë¡œë“œ íŒŒì¼ëª…ì— UUID ë¶€ì—¬ (ì¤‘ë³µ ë°©ì§€)
        filename = f"{uuid.uuid4()}_{file.filename}"
        bucket = "filedepot-bucket"
        key = filename
        chunk_size = 20 * 1024 * 1024  # 20MB

        # WHAT: í™˜ê²½ì— ë”°ë¼ ì‹¤ì œ S3 ë˜ëŠ” ë©”ëª¨ë¦¬ mock í´ë¼ì´ì–¸íŠ¸ ì„ íƒ
        # WHY: ìš´ì˜/ìŠ¤í…Œì´ì§€ëŠ” MinioProdClient, ê·¸ ì™¸ëŠ” MinioMemoryClient ì‚¬ìš©
        if settings.ENV in ["production", "stage"]:
            minio_client = MinioProdClient()
        else:
            minio_client = MinioMemoryClient()

        # WHAT: ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œíŒŒì¼ë¡œ ì €ì¥ (aiofiles í™œìš©)
        # WHY: ëŒ€ìš©ëŸ‰ íŒŒì¼ë„ chunk ë‹¨ìœ„ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
        temp_path = f"/tmp/{filename}"
        async with aiofiles.open(temp_path, 'wb') as out_file:
            while True:
                chunk = await file.read(chunk_size)
                if not chunk:
                    break
                await out_file.write(chunk)

        file_size = os.path.getsize(temp_path)
        from app.common.kafka.producer_factory import get_kafka_producer
        if file_size < chunk_size:
            # WHAT: ë‹¨ì¼ ì—…ë¡œë“œ(20MB ë¯¸ë§Œ) - ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ì 
            async with aiofiles.open(temp_path, 'rb') as f:
                data = await f.read()
                location = minio_client.upload_file(bucket, key, data)
                os.remove(temp_path)  # WHY: ì„ì‹œíŒŒì¼ ì‚­ì œë¡œ ìì› íšŒìˆ˜
                producer = get_kafka_producer()
                kafka_result = await producer.produce("file_metadata", {"filename": filename, "location": location})
                # WHAT: ì—…ë¡œë“œ í›„ ê²€ìˆ˜(ë¬´ê²°ì„± ê²€ì¦) ë¡œê¹…
                if settings.ENV in ["production", "stage"]:
                    # TODO: ì‹¤ì œ ê²€ìˆ˜/ë¬´ê²°ì„± ê²€ì‚¬ ì¸í„°í˜ì´ìŠ¤ í˜¸ì¶œ í•„ìš”
                    logger.info(f"[ê²€ìˆ˜] íŒŒì¼ {filename} ì—…ë¡œë“œ í›„ ë¬´ê²°ì„± ê²€ì¦ (ì‹¤ì„œë¹„ìŠ¤)")
                else:
                    logger.info(f"[ê²€ìˆ˜] íŒŒì¼ {filename} ì—…ë¡œë“œ í›„ ë¬´ê²°ì„± ê²€ì¦ (dummy)")
                return UploadResponse(filename=filename, status="uploaded", location=location, kafka_result=kafka_result)
        else:
            # WHAT: multipart upload (20MB ì´ìƒ) - chunk ë‹¨ìœ„ë¡œ ë¶„í•  ì—…ë¡œë“œ
            logger.info(f"[ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‹œì‘] íŒŒì¼ëª…={filename}, ì „ì²´ìš©ëŸ‰={file_size} bytes, chunk={chunk_size} bytes")
            # WHY: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ ì§„í–‰ë¥  ì‹œê°í™”
            uploaded = 0
            with open(temp_path, "rb") as f:
                part_num = 1
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    minio_client.upload_file(bucket, f"{key}.part{part_num}", chunk)
                    uploaded += len(chunk)
                    percent = int(uploaded / file_size * 100)
                    logger.info(f"[ì—…ë¡œë“œ ì§„í–‰] {uploaded}/{file_size} bytes ({percent}%) part={part_num}")
                    part_num += 1
            # WARNING: ì‹¤ì œë¡œëŠ” multipart_uploadë¡œ íŒŒíŠ¸ ë³‘í•©ì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ë‹¨ìˆœí™”
            location = f"memory://{bucket}/{key}" if settings.ENV not in ["production", "stage"] else f"s3://{bucket}/{key}"
            os.remove(temp_path)  # WHY: ì„ì‹œíŒŒì¼ ì‚­ì œë¡œ ìì› íšŒìˆ˜
            producer = KafkaMessageProducer()
            kafka_result = await producer.produce("file_metadata", {"filename": filename, "location": location})
            # WHAT: ì—…ë¡œë“œ í›„ ê²€ìˆ˜(ë¬´ê²°ì„± ê²€ì¦) ë¡œê¹…
            if settings.ENV in ["production", "stage"]:
                # TODO: ì‹¤ì œ ê²€ìˆ˜/ë¬´ê²°ì„± ê²€ì‚¬ ì¸í„°í˜ì´ìŠ¤ í˜¸ì¶œ í•„ìš”
                logger.info(f"[ê²€ìˆ˜] íŒŒì¼ {filename} ì—…ë¡œë“œ í›„ ë¬´ê²°ì„± ê²€ì¦ (ì‹¤ì„œë¹„ìŠ¤)")
            else:
                logger.info(f"[ê²€ìˆ˜] íŒŒì¼ {filename} ì—…ë¡œë“œ í›„ ë¬´ê²°ì„± ê²€ì¦ (dummy)")
            logger.info(f"[ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ] íŒŒì¼ëª…={filename}, ì „ì²´ìš©ëŸ‰={file_size} bytes")
            return UploadResponse(filename=filename, status="uploaded", location=location, kafka_result=kafka_result)


